{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import lit_ml_tools as lit\n",
    "\n",
    "# This will reload modules that have been edited\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to Test\n",
    "- neural net arg for colormap (line 198,355,380,396)\n",
    "        - 380: weights\n",
    "        - 396: biases\n",
    "- look at MLPClassifier\n",
    "- read about shapely values\n",
    "- start testing NNs\n",
    "- be able to explain *everything* - notes after each code block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "nentries = 10000\n",
    "nfeatures = 5\n",
    "\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='normal') #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Plots\n",
    "'''\n",
    "alldata,labels= lit.concat_dataset(dataset1, dataset2, wantplots=True)\n",
    "\n",
    "lit.sumfunc(dataset1)\n",
    "#lit.histfunc(dataset1)\n",
    "lit.sumfunc(dataset2)\n",
    "#lit.histfunc(dataset2)\n",
    "\n",
    "lit.correlations(dataset1, dataset2, label=0, colormap= plt.cm.Greens, wantplots=True, ax1=None)\n",
    "lit.correlations(dataset1, dataset2, label=1, colormap= plt.cm.Greens, wantplots=True, ax1=None)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nHL= 2    # number of HLs (only up to 2 for now)\n",
    "nnode= 4  # formerly n_arb\n",
    "ntrials= 2\n",
    "\n",
    "print(f'Best Node Pattern(s) for {nHL} Hidden Layers:')\n",
    "lit.best_node_pattern(dataset1,dataset2,nentries,nfeatures,nnode,ntrials,nHL, wantplots= True) ## only works for up to 2 HLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flattening Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excellent_training(dataset1, dataset2, num_hidden_layers= (nfeatures,3,8,4), colormap=plt.cm.Greens):\n",
    "    auc = 0\n",
    "    w,b = None,None\n",
    "\n",
    "    while auc < 0.9:\n",
    "        w, b, auc = lit.neuralnet(dataset1, dataset2, num_hidden_layers)\n",
    "\n",
    "        print(f\"auc: {auc}\")\n",
    "\n",
    "    # Draws NN after finding excellent training approach\n",
    "    lit.draw_network(b, w, ax=plt.gca(), colormap=colormap)\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bellis edits\n",
    "def flatten_2d_lists(arr2d):\n",
    "    vals = []\n",
    "    nrows = len(arr2d)\n",
    "    ncols = len(arr2d[0])\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            vals.append(arr2d[i][j])\n",
    "            \n",
    "    return vals\n",
    "\n",
    "# Flatten all the weights returned by a training \n",
    "def flatten_many_weights(weights):\n",
    "    all_weights = []\n",
    "    for weight in weights:\n",
    "        x = flatten_2d_lists(weight)\n",
    "        all_weights += x\n",
    "\n",
    "    return all_weights\n",
    "\n",
    "# Flatten all the biases together\n",
    "def flatten_many_biases(biases):\n",
    "    # These should be many 1d lists\n",
    "    all_biases = []\n",
    "    for bias in biases:\n",
    "        for i in range(len(bias)):\n",
    "            all_biases.append(bias[i])\n",
    "    return all_biases\n",
    "\n",
    "def merge_all_weights_and_biases(w,b):\n",
    "    all_w = flatten_many_weights(w)\n",
    "    all_b = flatten_many_biases(b)\n",
    "\n",
    "    all_the_things = all_w + all_b\n",
    "\n",
    "    return all_the_things\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_wb_dataset(dataset1, dataset2, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (100,10), gridsubplot= (50,20), nentries_wb=10):\n",
    "    # nentries_wb is for num of entries for w,b dataset\n",
    "\n",
    "    ## getting excellent NN grid and the w,b dataset\n",
    "    dataset= [] ##flattened\n",
    "\n",
    "    #fig= plt.figure(figsize=(nentries_wb*2, nentries_wb/2))\n",
    "    fig= plt.figure(figsize=gridfigsize)\n",
    "\n",
    "    for i in range(nentries_wb):\n",
    "        nrows,ncols= gridsubplot\n",
    "        plt.rcParams[\"figure.figsize\"] = [4, 3]\n",
    "        plt.subplot(nrows,ncols,i+1) #change this\n",
    "\n",
    "        w,b= excellent_training(dataset1, dataset2, num_hidden_layers,colormap=colormap)\n",
    "        vals = merge_all_weights_and_biases(w,b)\n",
    "        #entry= list_to_features(vals)\n",
    "        #dataset.append(entry)\n",
    "        dataset.append(vals)\n",
    "\n",
    "    fig.savefig(f\"NN_grid_of_{nentries_wb}\")\n",
    "    #plt.close()\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def whatdata(nentries, nfeatures, dtype= 'normal', diff_per_iter= False):\n",
    "    #dtype args: 'normal', 'squared', 'relativity'\n",
    "    #diff_per_iter= True generates a new dataset per iteration/for each entry; False if same dataset\n",
    "    dataset1= []\n",
    "    dataset2= []\n",
    "\n",
    "    if diff_per_iter == False:\n",
    "        dataset1 = lit.gen_original_data(nentries, nfeatures, dtype)\n",
    "        dataset2 = lit.shuffle_dataset(dataset1)\n",
    "\n",
    "    elif diff_per_iter == True:\n",
    "        dataset1_hold= []\n",
    "        for i in range(nentries):\n",
    "            dataset1x = lit.gen_original_data(1, nfeatures, dtype)\n",
    "            #print(dataset1x[0])\n",
    "            dataset1_hold.append(dataset1x[0].tolist())\n",
    "\n",
    "        dataset1= np.array(dataset1_hold)\n",
    "        dataset2 = lit.shuffle_dataset(dataset1)\n",
    "\n",
    "    return dataset1, dataset2\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def list_to_features(list):\n",
    "#    return [[el] for el in list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in a function\n",
    "   #dataset= []\n",
    " # for loop\n",
    "   # excellent_training - gives w,b\n",
    "   # merge_all_weights_and_biases - list with len 121\n",
    "   # list_to_features - list with 121 features\n",
    "   # dataset.append(list_to_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_wb_dataset(dataset1, dataset2, nentries_wb=10, num_hidden_layers= (nfeatures,3,8,4)):\n",
    "    dataset= []\n",
    "    \n",
    "    #fig= plt.figure(figsize=(nentries_wb*2, nentries_wb/2))\n",
    "    fig= plt.figure(figsize=(50,100))  \n",
    "    for i in range(nentries_wb):\n",
    "        plt.rcParams[\"figure.figsize\"] = [4, 3]\n",
    "        \n",
    "        plt.subplot(50,10,i+1)\n",
    "        #plt.subplot(2,3,i+1)\n",
    "\n",
    "        w,b= excellent_training(dataset1, dataset2, num_hidden_layers)\n",
    "        vals = merge_all_weights_and_biases(w,b)\n",
    "        #entry= list_to_features(vals)\n",
    "        #dataset.append(entry)\n",
    "        dataset.append(vals)\n",
    "    fig.savefig(f\"NN_grid_of_{nentries_wb}\")\n",
    "    #plt.close()\n",
    "    return dataset\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for everything\n",
    "nentries = 10000\n",
    "nfeatures = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUM TO ONE\n",
    "dataset1 = lit.gen_original_data(nentries, nfeatures, dtype='normal')  #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2 = lit.shuffle_dataset(dataset1)\n",
    "\n",
    "dataset_normal= get_wb_dataset(dataset1, dataset2, nentries_wb=1, num_hidden_layers= (nfeatures,3,8,4))\n",
    "dataset_normal= get_wb_dataset(dataset1, dataset2, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (100,10), gridsubplot= (50,20), nentries_wb=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RELATIVITY\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='relativity') #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "dataset_relativity= get_wb_dataset(dataset1, dataset2, nentries_wb=100, num_hidden_layers= (nfeatures,3,8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset1[0])\n",
    "print(dataset_normal[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('nentries:',len(dataset_normal),len(dataset_relativity))\n",
    "print('nfeatures:',len(dataset_normal[0]),len(dataset_relativity[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('shape of entry 1')\n",
    "#print(np.array(dataset_normal[0]).shape)\n",
    "#print(np.array(dataset_relativity[0]).shape)\n",
    "\n",
    "#print('how it should look')\n",
    "#print(dataset1[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## it doesn't need fixing\n",
    "#print('fixed shape of entry 1')\n",
    "#print(np.array([dataset_normal[0]]).T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('nentries, nfeatures')\n",
    "print(dataset1.shape)\n",
    "print(np.array(dataset_normal).shape)\n",
    "print(np.array(dataset_relativity).shape)\n",
    "print()\n",
    "print('shape of entry 1')\n",
    "print(dataset1[0].shape)\n",
    "print(np.array(dataset_normal[0]).shape)\n",
    "print(np.array(dataset_relativity[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures= len(dataset_normal[0])\n",
    "for i in range(15):\n",
    "    print(i)\n",
    "    #plt.figure()\n",
    "    excellent_training(np.array(dataset_normal), np.array(dataset_relativity), num_hidden_layers= (nfeatures,80,50,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding to get_wb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## want subplot size to be an arg\n",
    "## want option to use same dataset or diff dataset for each iteration\n",
    "\n",
    "##need to put data generation into get_wb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows,ncols=(50,10)\n",
    "print(nrows)\n",
    "print(ncols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wb_dataset(nentries, nfeatures, dtype= 'normal', num_hidden_layers= (nfeatures,3,8,4), colormap=plt.cm.Greens, gridfigsize= (100,10), gridsubplot= (50,20), nentries_wb=10, diff_per_iter= False):\n",
    "    # nentries_wb is for num of entries for w,b dataset\n",
    "\n",
    "    ##\n",
    "    if diff_per_iter == False:\n",
    "        dataset1 = lit.gen_original_data(nentries, nfeatures, dtype)\n",
    "        dataset2 = lit.shuffle_dataset(dataset1)\n",
    "\n",
    "        ## getting excellent NN grid and the w,b dataset\n",
    "        dataset= [] ##flattened\n",
    "        fig= plt.figure(figsize=gridfigsize)\n",
    "\n",
    "        for i in range(nentries_wb):\n",
    "            nrows,ncols= gridsubplot\n",
    "            plt.rcParams[\"figure.figsize\"] = [4, 3]\n",
    "            plt.subplot(nrows,ncols,i+1)\n",
    "\n",
    "            w,b= excellent_training(dataset1, dataset2, num_hidden_layers,colormap=colormap)\n",
    "            vals = merge_all_weights_and_biases(w,b)\n",
    "            dataset.append(vals)\n",
    "\n",
    "        fig.savefig(f\"NN_grid_of_{nentries_wb}\")\n",
    "\n",
    "    ##\n",
    "    elif diff_per_iter == True:\n",
    "        ## getting excellent NN grid and the w,b dataset\n",
    "        dataset= [] ##flattened\n",
    "        fig= plt.figure(figsize=gridfigsize)\n",
    "\n",
    "        for i in range(nentries_wb):\n",
    "            dataset1 = lit.gen_original_data(nentries, nfeatures, dtype)\n",
    "            dataset2 = lit.shuffle_dataset(dataset1)\n",
    "            nrows,ncols= gridsubplot\n",
    "            plt.rcParams[\"figure.figsize\"] = [4, 3]\n",
    "            plt.subplot(nrows,ncols,i+1)\n",
    "\n",
    "            w,b= excellent_training(dataset1, dataset2, num_hidden_layers,colormap=colormap)\n",
    "            vals = merge_all_weights_and_biases(w,b)\n",
    "            dataset.append(vals)\n",
    "\n",
    "        fig.savefig(f\"NN_grid_of_{nentries_wb}_{num_hidden_layers}\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whatdata(nentries, nfeatures, dtype= 'normal', diff_per_iter= False):\n",
    "    #dtype args: 'normal', 'squared', 'relativity' \n",
    "    #diff_per_iter= True generates a new dataset per iteration/for each entry; False if same dataset\n",
    "    dataset1= []\n",
    "    dataset2= []\n",
    "\n",
    "    if diff_per_iter == False:\n",
    "        dataset1 = lit.gen_original_data(nentries, nfeatures, dtype)  \n",
    "        dataset2 = lit.shuffle_dataset(dataset1)\n",
    "\n",
    "    elif diff_per_iter == True:\n",
    "        dataset1_hold= []\n",
    "        for i in range(nentries):\n",
    "            dataset1x = lit.gen_original_data(1, nfeatures, dtype)\n",
    "            #print(dataset1x[0])\n",
    "            dataset1_hold.append(dataset1x[0].tolist())\n",
    "\n",
    "        dataset1= np.array(dataset1_hold)\n",
    "        dataset2 = lit.shuffle_dataset(dataset1)\n",
    "\n",
    "    return dataset1, dataset2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for everything\n",
    "nentries = 10000\n",
    "nfeatures = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wn_same= get_wb_dataset(nentries, nfeatures, dtype= 'normal', num_hidden_layers= (nfeatures,8,5,2), colormap=plt.cm.Greens, gridfigsize= (5,5), gridsubplot= (1,1), nentries_wb=1, diff_per_iter= False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(wn_same[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wn_diff= get_wb_dataset(nentries, nfeatures, dtype= 'normal', num_hidden_layers= (nfeatures,3,8,4), colormap=plt.cm.Greens, gridfigsize= (5,5), gridsubplot= (1,1), nentries_wb=1, diff_per_iter= True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(wn_diff[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Testing wb datasets against each other\n",
    "nfeatures= len(wn_same[0])\n",
    "for i in range(15):\n",
    "    print(i)\n",
    "    #plt.figure()\n",
    "    excellent_training(np.array(wn_same), np.array(wn_diff), num_hidden_layers= (nfeatures,80,50,10))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating datasets\n",
    "dataset1_n, dataset2_n= whatdata(nentries, nfeatures, dtype= 'normal', diff_per_iter= False)\n",
    "#dataset1_r, dataset2_r= whatdata(nentries, nfeatures, dtype= 'relativity', diff_per_iter= False)\n",
    "#dataset1_s, dataset2_s= whatdata(nentries, nfeatures, dtype= 'squared', diff_per_iter= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the grids and w,b datasets\n",
    "wb_dataset_n= get_wb_dataset(dataset1_n, dataset2_n, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (20,10), gridsubplot= (2,5), nentries_wb=10,colormap=plt.cm.Greens)\n",
    "print('wb_dataset_n done')\n",
    "#wb_dataset_r= get_wb_dataset(dataset1_r, dataset2_r, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (50,100), gridsubplot= (50,10), nentries_wb=10)\n",
    "#print('wb_dataset_r done')\n",
    "#wb_dataset_s= get_wb_dataset(dataset1_s, dataset2_s, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (50,100), gridsubplot= (50,10), nentries_wb=10)\n",
    "#print('wb_dataset_s done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing wb datasets against each other\n",
    "nfeatures= len(dataset_normal[0])\n",
    "for i in range(15):\n",
    "    print(i)\n",
    "    #plt.figure()\n",
    "    excellent_training(np.array(dataset_normal), np.array(dataset_relativity), num_hidden_layers= (nfeatures,80,50,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excellent_training(dataset1, dataset2, num_hidden_layers= (nfeatures,3,8,4), colormap=plt.cm.Greens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nentries = 10000\n",
    "nfeatures = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal / Same \n",
    "dataset1_ns, dataset2_ns= whatdata(nentries, nfeatures, dtype= 'normal', diff_per_iter= False)\n",
    "get_wb_dataset(dataset1_ns, dataset2_ns, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (30,30), gridsubplot= (6,6), nentries_wb=36)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal / Diff\n",
    "dataset1_nd, dataset2_nd= whatdata(nentries, nfeatures, dtype= 'normal', diff_per_iter= True)\n",
    "get_wb_dataset(dataset1_nd, dataset2_nd, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (100,10), gridsubplot= (50,20), nentries_wb=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relativity / Same\n",
    "dataset1_rs, dataset2_rs= whatdata(nentries, nfeatures, dtype= 'relativity', diff_per_iter= False)\n",
    "get_wb_dataset(dataset1_rs, dataset2_rs, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (100,10), gridsubplot= (50,20), nentries_wb=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relativity / Diff\n",
    "dataset1_rd, dataset2_rd= whatdata(nentries, nfeatures, dtype= 'relativity', diff_per_iter= True)\n",
    "get_wb_dataset(dataset1_rd, dataset2_rd, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (100,10), gridsubplot= (50,20), nentries_wb=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared / Same\n",
    "dataset1_ss, dataset2_ss= whatdata(nentries, nfeatures, dtype= 'squared', diff_per_iter= False)\n",
    "#wb_ss= get_wb_dataset(dataset1_ss, dataset2_ss, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,5,4,2), gridfigsize= (30,30), gridsubplot= (9,3), nentries_wb=9)\n",
    "\n",
    "nHL= 2    # number of HLs (only up to 2 for now)\n",
    "nnode= 8  # formerly n_arb\n",
    "ntrials= 2\n",
    "\n",
    "print(f'Best Node Pattern(s) for {nHL} Hidden Layers:')\n",
    "lit.best_node_pattern(dataset1_ss,dataset2_ss,nentries,nfeatures,nnode,ntrials,nHL, wantplots= True) ## only works for up to 2 HLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_ss= get_wb_dataset(dataset1_ss, dataset2_ss, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,8,5), gridfigsize= (30,30), gridsubplot= (9,3), nentries_wb=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating and Saving BIG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same Dataset per Iteration\n",
    "dataset1_ns, dataset2_ns= whatdata(nentries, nfeatures, dtype= 'normal', diff_per_iter= False)\n",
    "dataset1_rs, dataset2_rs= whatdata(nentries, nfeatures, dtype= 'relativity', diff_per_iter= False)\n",
    "dataset1_ss, dataset2_ss= whatdata(nentries, nfeatures, dtype= 'squared', diff_per_iter= False)\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "## Diff Dataset per Iteration\n",
    "dataset1_nd, dataset2_nd= whatdata(nentries, nfeatures, dtype= 'normal', diff_per_iter= True)\n",
    "dataset1_rd, dataset2_rd= whatdata(nentries, nfeatures, dtype= 'relativity', diff_per_iter= True)\n",
    "dataset1_sd, dataset2_sd= whatdata(nentries, nfeatures, dtype= 'squared', diff_per_iter= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing\n",
    "dataset1_t, dataset2_t= whatdata(nentries, nfeatures, dtype= 'normal', diff_per_iter= False)\n",
    "\n",
    "wb_test= get_wb_dataset(dataset1_t, dataset2_t, nentries_wb=10, num_hidden_layers= (nfeatures,3,8,4))\n",
    "file = open('wb_normal_test.pkl', 'wb')\n",
    "pickle.dump(wb_test, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset1_t)\n",
    "print(dataset2_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_testout = pickle.load(open('wb_normal_test.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_testout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit.draw_network(dataset1_t, dataset2_t, figsize=(6, 6), colormap=plt.cm.Greens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WB Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3x2 Grid \n",
    "\n",
    "## Same Dataset per Iteration\n",
    "'''\n",
    "wb_ns= get_wb_dataset(dataset1_ns, dataset2_ns, nentries_wb=1000, num_hidden_layers= (nfeatures,3,8,4))\n",
    "file = open('wb_normal_same.pkl', 'wb')\n",
    "pickle.dump(wb_ns, file)\n",
    "file.close()\n",
    "print('1 done')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "wb_rs= get_wb_dataset(dataset1_rs, dataset2_rs, nentries_wb=1000, num_hidden_layers= (nfeatures,3,8,4))\n",
    "file = open('wb_relativity_same.pkl', 'wb')\n",
    "pickle.dump(wb_rs, file)\n",
    "file.close()\n",
    "print('2 done')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running second to last\n",
    "wb_ss= get_wb_dataset(dataset1_ss, dataset2_ss, nentries_wb=1000, num_hidden_layers= (nfeatures,3,8,4))\n",
    "file = open('wb_squared_same.pkl', 'wb')\n",
    "pickle.dump(wb_ss, file)\n",
    "file.close()\n",
    "print('3 done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Diff Dataset per Iteration\n",
    "wb_nd= get_wb_dataset(dataset1_nd, dataset2_nd, nentries_wb=1000, num_hidden_layers= (nfeatures,3,8,4))\n",
    "file = open('wb_normal_diff.pkl', 'wb')\n",
    "pickle.dump(wb_nd, file)\n",
    "file.close()\n",
    "print('4 done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_rd= get_wb_dataset(dataset1_rd, dataset2_rd, nentries_wb=1000, num_hidden_layers= (nfeatures,3,8,4))\n",
    "file = open('wb_relativity_diff.pkl', 'wb')\n",
    "pickle.dump(wb_rd, file)\n",
    "file.close()\n",
    "print('5 done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## running last\n",
    "wb_sd= get_wb_dataset(dataset1_sd, dataset2_sd, nentries_wb=1000, num_hidden_layers= (nfeatures,3,8,4))\n",
    "file = open('wb_squared_diff.pkl', 'wb')\n",
    "pickle.dump(wb_sd, file)\n",
    "file.close()\n",
    "print('6 done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('completely done')\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WB against WB (3x2 grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "##this should be excellent_training with same datasets as below\n",
    "## Same Dataset per Iteration\n",
    "# normal - relativity\n",
    "get_wb_dataset(wb_ns, wb_rs, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (50,100), gridsubplot= (50,10), nentries_wb=10)\n",
    "\n",
    "# relativity - squared\n",
    "get_wb_dataset(wb_rs, wb_ss, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (50,100), gridsubplot= (50,10), nentries_wb=10)\n",
    "\n",
    "# squared - normal\n",
    "get_wb_dataset(wb_ss, wb_ns, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (50,100), gridsubplot= (50,10), nentries_wb=10)\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "## Diff Dataset per Iteration\n",
    "# normal - relativity\n",
    "get_wb_dataset(wb_nd, wb_rd, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (50,100), gridsubplot= (50,10), nentries_wb=10)\n",
    "\n",
    "# relativity - squared\n",
    "get_wb_dataset(wb_rd, wb_sd, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (50,100), gridsubplot= (50,10), nentries_wb=10)\n",
    "\n",
    "# squared - normal\n",
    "get_wb_dataset(wb_sd, wb_nd, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (50,100), gridsubplot= (50,10), nentries_wb=10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gabby Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wb_dataset_09(dataset1, dataset2, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (50,100), gridsubplot= (50,10), nentries_wb=10):\n",
    "    # nentries_wb is for num of entries for w,b dataset\n",
    "\n",
    "    ## getting excellent NN grid and the w,b dataset\n",
    "    dataset= [] ##flattened\n",
    "\n",
    "    #fig= plt.figure(figsize=(nentries_wb*2, nentries_wb/2))\n",
    "    fig= plt.figure(figsize=gridfigsize)\n",
    "\n",
    "    for i in range(nentries_wb):\n",
    "        nrows,ncols= gridsubplot\n",
    "        plt.rcParams[\"figure.figsize\"] = [4, 3]\n",
    "        plt.subplot(nrows,ncols,i+1) #change this\n",
    "\n",
    "        w,b= excellent_training_09(dataset1, dataset2, num_hidden_layers,colormap=colormap)\n",
    "        vals = merge_all_weights_and_biases(w,b)\n",
    "        #entry= list_to_features(vals)\n",
    "        #dataset.append(entry)\n",
    "        dataset.append(vals)\n",
    "\n",
    "    fig.savefig(f\"NN_grid_of_{nentries_wb}\")\n",
    "    #plt.close()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_wb_dataset_06(dataset1, dataset2, colormap=plt.cm.Greens, num_hidden_layers= (nfeatures,3,8,4), gridfigsize= (50,100), gridsubplot= (50,10), nentries_wb=10):\n",
    "    # nentries_wb is for num of entries for w,b dataset\n",
    "\n",
    "    ## getting excellent NN grid and the w,b dataset\n",
    "    dataset= [] ##flattened\n",
    "\n",
    "    #fig= plt.figure(figsize=(nentries_wb*2, nentries_wb/2))\n",
    "    fig= plt.figure(figsize=gridfigsize)\n",
    "\n",
    "    for i in range(nentries_wb):\n",
    "        nrows,ncols= gridsubplot\n",
    "        plt.rcParams[\"figure.figsize\"] = [4, 3]\n",
    "        plt.subplot(nrows,ncols,i+1) #change this\n",
    "\n",
    "        w,b= excellent_training_06(dataset1, dataset2, num_hidden_layers,colormap=colormap)\n",
    "        vals = merge_all_weights_and_biases(w,b)\n",
    "        #entry= list_to_features(vals)\n",
    "        #dataset.append(entry)\n",
    "        dataset.append(vals)\n",
    "\n",
    "    fig.savefig(f\"NN_grid_of_{nentries_wb}\")\n",
    "    #plt.close()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excellent_training_09(dataset1, dataset2, num_hidden_layers= (nfeatures,3,8,4), colormap=plt.cm.Greens):\n",
    "    auc = 0\n",
    "    w,b = None,None\n",
    "\n",
    "    while auc < 0.9:\n",
    "        w, b, auc = lit.neuralnet(dataset1, dataset2, num_hidden_layers)\n",
    "\n",
    "        print(f\"auc: {auc}\")\n",
    "\n",
    "    # Draws NN after finding excellent training approach\n",
    "    lit.draw_network(b, w, ax=plt.gca(), colormap=colormap)\n",
    "    return w,b\n",
    "\n",
    "def excellent_training_06(dataset1, dataset2, num_hidden_layers= (nfeatures,3,8,4), colormap=plt.cm.Greens):\n",
    "    auc = 0\n",
    "    w,b = None,None\n",
    "\n",
    "    while auc > 0.6:\n",
    "        w, b, auc = lit.neuralnet(dataset1, dataset2, num_hidden_layers)\n",
    "\n",
    "        print(f\"auc: {auc}\")\n",
    "\n",
    "    # Draws NN after finding excellent training approach\n",
    "    lit.draw_network(b, w, ax=plt.gca(), colormap=colormap)\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for everything\n",
    "nentries = 10000\n",
    "nfeatures = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUM TO ONE (1)\n",
    "dataset1 = lit.gen_original_data(nentries, nfeatures, dtype='normal')  #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2 = lit.shuffle_dataset(dataset1)\n",
    "\n",
    "dataset_normal09= get_wb_dataset_09(dataset1, dataset2, nentries_wb=500, num_hidden_layers= (nfeatures,3,8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUM TO ONE (2)\n",
    "dataset1 = lit.gen_original_data(nentries, nfeatures, dtype='normal')  #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2 = lit.shuffle_dataset(dataset1)\n",
    "\n",
    "dataset_normal06= get_wb_dataset_06(dataset1, dataset2, nentries_wb=500, num_hidden_layers= (nfeatures,3,8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## can NN distinguish between WB from sumtoone auc<0.6 and auc>0.9 ??\n",
    "\n",
    "nfeatures= len(dataset_normal09[0])\n",
    "for i in range(15):\n",
    "    print(i)\n",
    "    #plt.figure()\n",
    "    excellent_training(np.array(dataset_normal09), np.array(dataset_normal06), num_hidden_layers= (nfeatures,80,50,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## can NN distinguish between WB from same sumtoone and diff sumtoone??\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing output to a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_normal1))\n",
    "print(len(dataset_normal1[0]))\n",
    "\n",
    "outfilename = \"dset_normal1.csv\"\n",
    "\n",
    "output = \"\"\n",
    "nrows = len(dataset_normal1)\n",
    "for i in range(nrows):\n",
    "    line = \",\".join(np.array(dataset_normal1[i]).astype(str))\n",
    "    output += line+\"\\n\"\n",
    "    \n",
    "outfile = open(outfilename,\"w\")\n",
    "outfile.write(output)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.loadtxt(outfilename,delimiter=',', skiprows=0, dtype=float, unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save some stuff as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we have a w and a b\n",
    "print(w)\n",
    "print(b)\n",
    "\n",
    "# open a file, where you ant to store the data\n",
    "file = open('pickle_test.pkl', 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump([w,b], file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back in the pickle\n",
    "wnew,bnew = pickle.load(open('pickle_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wnew)\n",
    "print(bnew)\n",
    "\n",
    "lit.draw_network(bnew, wnew, figsize=(6, 6), colormap=plt.cm.Greens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making sure we get a good training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Manual Variables\n",
    "nentries = 10000\n",
    "nfeatures = 4\n",
    "\n",
    "# Datasets\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='normal') #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "alldata,labels= lit.concat_dataset(dataset1, dataset2, wantplots=False)\n",
    "\n",
    "auc = 0\n",
    "w,b = None,None\n",
    "\n",
    "while auc < 0.9:\n",
    "\n",
    "    w, b, auc = lit.neuralnet(dataset1, dataset2, num_hidden_layers=(nfeatures, 3, 8, 4), wantplots=False)\n",
    "\n",
    "    print(f\"auc: {auc}\")\n",
    "\n",
    "# Only draw the network after it find a good training approach\n",
    "lit.draw_network(b, w, figsize=(6, 6), colormap=plt.cm.Greens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Manual Variables\n",
    "nentries = 10000\n",
    "nfeatures = 4\n",
    "\n",
    "# Datasets\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='relativity') #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "alldata,labels= lit.concat_dataset(dataset1, dataset2, wantplots=False)\n",
    "\n",
    "auc = 0\n",
    "w,b = None,None\n",
    "\n",
    "while auc < 0.9:\n",
    "\n",
    "    w, b, auc = lit.neuralnet(dataset1, dataset2, num_hidden_layers=(nfeatures, 3, 8, 4), wantplots=False)\n",
    "\n",
    "    print(f\"auc: {auc}\")\n",
    "\n",
    "# Only draw the network after it find a good training approach\n",
    "lit.draw_network(b, w, figsize=(6, 6), colormap=plt.cm.Greens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
