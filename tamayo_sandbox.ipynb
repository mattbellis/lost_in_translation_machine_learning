{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import lit_ml_tools as lit\n",
    "\n",
    "# This will reload modules that have been edited\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to Test\n",
    "- neural net arg for colormap (line 198,355,380,396)\n",
    "        - 380: weights\n",
    "        - 396: biases\n",
    "- look at MLPClassifier\n",
    "- read about shapely values\n",
    "- start testing NNs\n",
    "- be able to explain *everything* - notes after each code block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Datasets\n",
    "nentries = 10000\n",
    "nfeatures = 5\n",
    "\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='normal') #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Additional Plots\n",
    "'''\n",
    "alldata,labels= lit.concat_dataset(dataset1, dataset2, wantplots=True)\n",
    "\n",
    "lit.sumfunc(dataset1)\n",
    "#lit.histfunc(dataset1)\n",
    "lit.sumfunc(dataset2)\n",
    "#lit.histfunc(dataset2)\n",
    "\n",
    "lit.correlations(dataset1, dataset2, label=0, colormap= plt.cm.Greens, wantplots=True, ax1=None)\n",
    "lit.correlations(dataset1, dataset2, label=1, colormap= plt.cm.Greens, wantplots=True, ax1=None)\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nHL= 2    # number of HLs (only up to 2 for now)\n",
    "nnode= 4  # formerly n_arb\n",
    "ntrials= 2\n",
    "\n",
    "print(f'Best Node Pattern(s) for {nHL} Hidden Layers:')\n",
    "lit.best_node_pattern(dataset1,dataset2,nentries,nfeatures,nnode,ntrials,nHL, wantplots= False) ## only works for up to 2 HLs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flattening Approach"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def excellent_training(dataset1, dataset2, num_hidden_layers= (nfeatures,3,8,4)):\n",
    "    auc = 0\n",
    "    w,b = None,None\n",
    "\n",
    "    while auc < 0.9:\n",
    "        w, b, auc = lit.neuralnet(dataset1, dataset2, num_hidden_layers)\n",
    "\n",
    "        print(f\"auc: {auc}\")\n",
    "\n",
    "    # Draws NN after finding excellent training approach\n",
    "    lit.draw_network(b, w, figsize=(6, 6), colormap=plt.cm.Greens)\n",
    "    return w,b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Variables for everything\n",
    "nentries = 10000\n",
    "nfeatures = 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## SUM TO ONE\n",
    "\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='normal') #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "w,b= excellent_training(dataset1, dataset2, num_hidden_layers= (nfeatures,3,8,4))\n",
    "\n",
    "w1_list= []\n",
    "b1_list= []\n",
    "\n",
    "for i in range(len(w)):\n",
    "    nrows,ncols= w[i].shape\n",
    "    for j in range(nrows):\n",
    "        for k in range(ncols):\n",
    "            w1_list.append(w[i][j][k])\n",
    "\n",
    "for i in range(len(b)):\n",
    "    nrows,ncols= np.array([b[i]]).T.shape\n",
    "    for j in range(nrows):\n",
    "        b1_list.append(b[i][j])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## RELATIVITY\n",
    "\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='relativity') #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "w,b= excellent_training(dataset1, dataset2, num_hidden_layers= (nfeatures,3,8,4))\n",
    "\n",
    "w2_list= []\n",
    "b2_list= []\n",
    "\n",
    "for i in range(len(w)):\n",
    "    nrows,ncols= w[i].shape\n",
    "    for j in range(nrows):\n",
    "        for k in range(ncols):\n",
    "            w2_list.append(w[i][j][k])\n",
    "\n",
    "for i in range(len(b)):\n",
    "    nrows,ncols= np.array([b[i]]).T.shape\n",
    "    for j in range(nrows):\n",
    "        b2_list.append(b[i][j])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#'''\n",
    "q1= [1,2,3]\n",
    "q2= [5,6]\n",
    "q=[q1,q2]\n",
    "\n",
    "k1= [9,8,7]\n",
    "k2= [0,1]\n",
    "k=[k1,k2]\n",
    "\n",
    "print(np.concatenate([q[0],w[0]]))\n",
    "print(q1+k1+q2+k2)\n",
    "print(q+k)\n",
    "#'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset1= [w1_list, b1_list]\n",
    "dataset2= [w2_list, b2_list]\n",
    "excellent_training(dataset1, dataset2, num_hidden_layers= (nfeatures,3,8,4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.concatenate([dataset1,dataset2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gabby making things really complicated Approach"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Variables for everything\n",
    "nentries = 10000\n",
    "nfeatures = 5\n",
    "ntrials= 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Sum to One\n",
    "print('Sum to One')\n",
    "\n",
    "biases1= []\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='normal')\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "for i in range(ntrials):\n",
    "    w, b= excellent_training(dataset1, dataset2)\n",
    "    biases1.append(b)\n",
    "b1_lists= list(zip(*biases1))\n",
    "\n",
    "## Relativity\n",
    "print('Relativity')\n",
    "\n",
    "biases2= []\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='relativity')\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "for i in range(ntrials):\n",
    "    w, b= excellent_training(dataset1, dataset2)\n",
    "    biases2.append(b)\n",
    "b2_lists= list(zip(*biases2))\n",
    "\n",
    "## NN\n",
    "#for i in range(nfeatures):\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## NN\n",
    "#for i in range(nfeatures):\n",
    "    #print(i)\n",
    "    #print(b1_lists[i])\n",
    "    #print()\n",
    "    #excellent_training(b1_lists[i], b2_lists[i])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sum to One\n",
    "print('Sum to One')\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='normal')\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "w1, b1= excellent_training(dataset1, dataset2)\n",
    "\n",
    "# Relativity\n",
    "print('Relativity')\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='relativity')\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "w2, b2= excellent_training(dataset1, dataset2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "b1_lists[0][0]\n",
    "b2_lists[0][0]\n",
    "dataset1[0:10][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#excellent_training(np.array(b1_lists[0]), np.array(b2_lists[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making sure we get a good training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Manual Variables\n",
    "nentries = 10000\n",
    "nfeatures = 4\n",
    "\n",
    "# Datasets\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='normal') #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "alldata,labels= lit.concat_dataset(dataset1, dataset2, wantplots=False)\n",
    "\n",
    "auc = 0\n",
    "w,b = None,None\n",
    "\n",
    "while auc < 0.9:\n",
    "\n",
    "    w, b, auc = lit.neuralnet(dataset1, dataset2, num_hidden_layers=(nfeatures, 3, 8, 4), wantplots=False)\n",
    "\n",
    "    print(f\"auc: {auc}\")\n",
    "\n",
    "# Only draw the network after it find a good training approach\n",
    "lit.draw_network(b, w, figsize=(6, 6), colormap=plt.cm.Greens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Manual Variables\n",
    "nentries = 10000\n",
    "nfeatures = 4\n",
    "\n",
    "# Datasets\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='relativity') #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "alldata,labels= lit.concat_dataset(dataset1, dataset2, wantplots=False)\n",
    "\n",
    "auc = 0\n",
    "w,b = None,None\n",
    "\n",
    "while auc < 0.9:\n",
    "\n",
    "    w, b, auc = lit.neuralnet(dataset1, dataset2, num_hidden_layers=(nfeatures, 3, 8, 4), wantplots=False)\n",
    "\n",
    "    print(f\"auc: {auc}\")\n",
    "\n",
    "# Only draw the network after it find a good training approach\n",
    "lit.draw_network(b, w, figsize=(6, 6), colormap=plt.cm.Greens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
