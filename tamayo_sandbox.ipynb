{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import lit_ml_tools as lit\n",
    "\n",
    "# This will reload modules that have been edited\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to Test\n",
    "- neural net arg for colormap (line 198,355,380,396)\n",
    "        - 380: weights\n",
    "        - 396: biases\n",
    "- look at MLPClassifier\n",
    "- read about shapely values\n",
    "- start testing NNs\n",
    "- be able to explain *everything* - notes after each code block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Variables\n",
    "nentries = 10000\n",
    "nfeatures = 5\n",
    "\n",
    "# Datasets\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='normal') #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "alldata,labels= lit.concat_dataset(dataset1, dataset2, wantplots=False)\n",
    "\n",
    "lit.sumfunc(dataset1);  #A histogram of the sum of each row for nentries rows. Each row sums to one.\n",
    "lit.histfunc(dataset1); #Histograms of each feature.\n",
    "\n",
    "lit.sumfunc(dataset2);  #Because each feature was shuffled, the histogram of the sum of each row now forms a gaussian peaking at approximately one.\n",
    "lit.histfunc(dataset2); #Each feature was shuffled within itself. Column of feature one shuffled, column of feature two shuffled, etc.. The histograms of each feature remains the same bc it is the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lit.correlations(dataset1, dataset2, label=0, colormap= plt.cm.Greens, wantplots=True, ax1=None)\n",
    "#lit.correlations(dataset1, dataset2, label=1, colormap= plt.cm.Greens, wantplots=True, ax1=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, roc = lit.neuralnet(dataset1, dataset2, num_hidden_layers=(nfeatures, nfeatures+2, nfeatures, nfeatures-2), wantplots=True)\n",
    "lit.draw_network(b, w, figsize=(6, 6), colormap=plt.cm.Greens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ROC classifications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2935260/#:~:text=The%20area%20under%20the%20ROC,AUC%20values%20between%200.5%2D0.6.)\n",
    "#### *how efficient the NN is*\n",
    "\n",
    "We extract the AUC (area under the (ROC) curve)\n",
    "\n",
    "excellent: 0.9-1\n",
    "good:      0.8-0.9\n",
    "fair:      0.7-0.8\n",
    "poor:      0.6-0.7\n",
    "failed:    0.5-0.6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_data(roc):\n",
    "    roc_scores= []\n",
    "    roc_values= []\n",
    "\n",
    "    if roc >= 0.9:\n",
    "        roc_scores.append('excellent')\n",
    "        roc_values.append(roc)\n",
    "    elif 0.8 <= roc < 0.9:\n",
    "        roc_scores.append('good')\n",
    "        roc_values.append(roc)\n",
    "    elif 0.7 <= roc < 0.8:\n",
    "        roc_scores.append('fair')\n",
    "        roc_values.append(roc)\n",
    "    elif 0.6 <= roc < 0.7:\n",
    "        roc_scores.append('poor')\n",
    "        roc_values.append(roc)\n",
    "    elif roc < 0.6:\n",
    "        roc_scores.append('failed')\n",
    "        roc_values.append(roc)\n",
    "\n",
    "    return roc_scores, roc_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##generate n_arb amount of nodes for one HL\n",
    "## should change n_arb to something like nnodes\n",
    "n_arb= 8\n",
    "roc_scores= []\n",
    "roc_values= []\n",
    "\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "for i in range(n_arb):\n",
    "    plt.subplot(4,4,i+1)\n",
    "\n",
    "    w,b,roc= lit.neuralnet(dataset1, dataset2, num_hidden_layers=(nfeatures, i+1), wantplots=False)\n",
    "    lit.draw_network(b,w, ax=plt.gca(), colormap=plt.cm.Greens)\n",
    "\n",
    "    roc_scores.append(ROC_data(roc)[0])\n",
    "    roc_values.append(ROC_data(roc)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_scores) ##can have this be printed on each diagram?\n",
    "print(roc_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##added multiple trials\n",
    "n_arb= 5\n",
    "ntrials= 4\n",
    "\n",
    "roc_scores= []\n",
    "roc_values= []\n",
    "\n",
    "for j in range(ntrials):\n",
    "    fig = plt.figure(figsize=(ntrials*n_arb,n_arb+n_arb))\n",
    "\n",
    "    roc_scores_pertrial=[]\n",
    "    roc_values_pertrial=[]\n",
    "    roc_scores.append(roc_scores_pertrial)\n",
    "    roc_values.append(roc_values_pertrial)\n",
    "\n",
    "    for i in range(n_arb):\n",
    "        plt.subplot(ntrials,n_arb,i+1)\n",
    "\n",
    "        w,b,roc= lit.neuralnet(dataset1, dataset2, num_hidden_layers=(nfeatures, i+1), wantplots=False)\n",
    "        lit.draw_network(b,w, ax=plt.gca(), colormap=plt.cm.Greens)\n",
    "\n",
    "        roc_scores_pertrial.append(ROC_data(roc)[0])\n",
    "        roc_values_pertrial.append(ROC_data(roc)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narb_list= []\n",
    "for i in range(n_arb):\n",
    "    narb_list.append(i+1)\n",
    "\n",
    "print('Number of Nodes:',narb_list)\n",
    "for i in range(ntrials):\n",
    "    print(roc_scores[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arb= 5\n",
    "ntrials= 2\n",
    "\n",
    "roc_scores= []\n",
    "roc_values= []\n",
    "HL_nodes= []\n",
    "\n",
    "for j in range(ntrials):\n",
    "\n",
    "    roc_scores_pertrial=[]\n",
    "    roc_values_pertrial=[]\n",
    "    HL_nodes_pertrial= []\n",
    "\n",
    "    roc_scores.append(roc_scores_pertrial)\n",
    "    roc_values.append(roc_values_pertrial)\n",
    "    HL_nodes.append(HL_nodes_pertrial)\n",
    "\n",
    "    for hl1 in range(n_arb):\n",
    "        for hl2 in range(n_arb):\n",
    "            plt.figure(figsize=(7,4))\n",
    "\n",
    "            w,b,roc= lit.neuralnet(dataset1, dataset2, num_hidden_layers=(nfeatures, hl1+1, hl2+1), wantplots=False)\n",
    "            lit.draw_network(b,w, ax=plt.gca(), colormap=plt.cm.Greens)\n",
    "\n",
    "            roc_scores_pertrial.append(ROC_data(roc)[0])\n",
    "            roc_values_pertrial.append(ROC_data(roc)[1])\n",
    "            HL_nodes_pertrial.append([hl1+1, hl2+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sorting node patterns\n",
    "excellent= []\n",
    "good= []\n",
    "fair= []\n",
    "poor= []\n",
    "failed= []\n",
    "scores_and_nodes= [excellent,good,fair,poor,failed]\n",
    "\n",
    "for t in range(ntrials):\n",
    "    for i in range(len(HL_nodes[t])):\n",
    "        #print(HL_nodes[t][i],roc_scores[t][i])\n",
    "        if roc_scores[t][i] == ['excellent']:\n",
    "            excellent.append(HL_nodes[t][i])\n",
    "        elif roc_scores[t][i] == ['good']:\n",
    "            good.append(HL_nodes[t][i])\n",
    "        elif roc_scores[t][i] == ['fair']:\n",
    "            fair.append(HL_nodes[t][i])\n",
    "        elif roc_scores[t][i] == ['poor']:\n",
    "            poor.append(HL_nodes[t][i])\n",
    "        elif roc_scores[t][i] == ['failed']:\n",
    "            failed.append(HL_nodes[t][i])\n",
    "\n",
    "print('Should be Equal:',len(excellent)+len(good)+len(fair)+len(poor)+len(failed),',', len(HL_nodes[0]*ntrials))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How i did what is below](https://www.trainingint.com/how-to-find-duplicates-in-a-python-list.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for repeats of node patterns\n",
    "# Unique lists\n",
    "u_excellent= []\n",
    "u_good= []\n",
    "u_fair= []\n",
    "u_poor= []\n",
    "u_failed= []\n",
    "\n",
    "# Repeat lists\n",
    "r_excellent= []\n",
    "r_good= []\n",
    "r_fair= []\n",
    "r_poor= []\n",
    "r_failed= []\n",
    "\n",
    "for i in range(len(scores_and_nodes)):  # cycle through each class\n",
    "    for j in scores_and_nodes[i]: # cycle through each element in each class\n",
    "        if i == 0:\n",
    "            if j not in u_excellent:\n",
    "                u_excellent.append(j)\n",
    "            else:\n",
    "                r_excellent.append(j)\n",
    "        elif i == 1:\n",
    "            if j not in u_good:\n",
    "                u_good.append(j)\n",
    "            else:\n",
    "                r_good.append(j)\n",
    "        elif i == 2:\n",
    "            if j not in u_fair:\n",
    "                u_fair.append(j)\n",
    "            else:\n",
    "                r_fair.append(j)\n",
    "        elif i == 3:\n",
    "            if j not in u_poor:\n",
    "                u_poor.append(j)\n",
    "            else:\n",
    "                r_poor.append(j)\n",
    "\n",
    "        elif i == 4:\n",
    "            if j not in u_failed:\n",
    "                u_failed.append(j)\n",
    "            else:\n",
    "                r_failed.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IT WORKS !!!!\n",
    "#print(excellent)\n",
    "print('best of best:',r_excellent,'\\n')\n",
    "#print(failed)\n",
    "print('worst of worst:',r_failed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
