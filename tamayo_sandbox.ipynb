{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import lit_ml_tools as lit\n",
    "\n",
    "# This will reload modules that have been edited\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to Test\n",
    "- neural net arg for colormap (line 198,355,380,396)\n",
    "        - 380: weights\n",
    "        - 396: biases\n",
    "- look at MLPClassifier\n",
    "- read about shapely values\n",
    "- start testing NNs\n",
    "- be able to explain *everything* - notes after each code block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "nentries = 10000\n",
    "nfeatures = 5\n",
    "\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='normal') #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Plots\n",
    "'''\n",
    "alldata,labels= lit.concat_dataset(dataset1, dataset2, wantplots=True)\n",
    "\n",
    "lit.sumfunc(dataset1)\n",
    "#lit.histfunc(dataset1)\n",
    "lit.sumfunc(dataset2)\n",
    "#lit.histfunc(dataset2)\n",
    "\n",
    "lit.correlations(dataset1, dataset2, label=0, colormap= plt.cm.Greens, wantplots=True, ax1=None)\n",
    "lit.correlations(dataset1, dataset2, label=1, colormap= plt.cm.Greens, wantplots=True, ax1=None)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nHL= 2    # number of HLs (only up to 2 for now)\n",
    "nnode= 4  # formerly n_arb\n",
    "ntrials= 2\n",
    "\n",
    "print(f'Best Node Pattern(s) for {nHL} Hidden Layers:')\n",
    "lit.best_node_pattern(dataset1,dataset2,nentries,nfeatures,nnode,ntrials,nHL, wantplots= True) ## only works for up to 2 HLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flattening Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excellent_training(dataset1, dataset2, num_hidden_layers= (nfeatures,3,8,4)):\n",
    "    auc = 0\n",
    "    w,b = None,None\n",
    "\n",
    "    while auc < 0.9:\n",
    "        w, b, auc = lit.neuralnet(dataset1, dataset2, num_hidden_layers)\n",
    "\n",
    "        print(f\"auc: {auc}\")\n",
    "\n",
    "    # Draws NN after finding excellent training approach\n",
    "    lit.draw_network(b, w, ax=plt.gca(), colormap=plt.cm.Greens)\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bellis edits\n",
    "def flatten_2d_lists(arr2d):\n",
    "    vals = []\n",
    "    nrows = len(arr2d)\n",
    "    ncols = len(arr2d[0])\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            vals.append(arr2d[i][j])\n",
    "            \n",
    "    return vals\n",
    "\n",
    "# Flatten all the weights returned by a training \n",
    "def flatten_many_weights(weights):\n",
    "    all_weights = []\n",
    "    for weight in weights:\n",
    "        x = flatten_2d_lists(weight)\n",
    "        all_weights += x\n",
    "\n",
    "    return all_weights\n",
    "\n",
    "# Flatten all the biases together\n",
    "def flatten_many_biases(biases):\n",
    "    # These should be many 1d lists\n",
    "    all_biases = []\n",
    "    for bias in biases:\n",
    "        for i in range(len(bias)):\n",
    "            all_biases.append(bias[i])\n",
    "    return all_biases\n",
    "\n",
    "def merge_all_weights_and_biases(w,b):\n",
    "    all_w = flatten_many_weights(w)\n",
    "    all_b = flatten_many_biases(b)\n",
    "\n",
    "    all_the_things = all_w + all_b\n",
    "\n",
    "    return all_the_things\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def list_to_features(list):\n",
    "#    return [[el] for el in list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in a function\n",
    "   #dataset= []\n",
    " # for loop\n",
    "   # excellent_training - gives w,b\n",
    "   # merge_all_weights_and_biases - list with len 121\n",
    "   # list_to_features - list with 121 features\n",
    "   # dataset.append(list_to_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wb_dataset(dataset1, dataset2, nentries_wb=10, num_hidden_layers= (nfeatures,3,8,4)):\n",
    "    dataset= []\n",
    "    \n",
    "    #fig= plt.figure(figsize=(nentries_wb*2, nentries_wb/2))\n",
    "    fig= plt.figure(figsize=(50,100))\n",
    "    for i in range(nentries_wb):\n",
    "        plt.rcParams[\"figure.figsize\"] = [4, 3]\n",
    "        \n",
    "        plt.subplot(20,5,i+1)\n",
    "        #plt.subplot(2,3,i+1)\n",
    "\n",
    "        w,b= excellent_training(dataset1, dataset2, num_hidden_layers)\n",
    "        vals = merge_all_weights_and_biases(w,b)\n",
    "        #entry= list_to_features(vals)\n",
    "        #dataset.append(entry)\n",
    "        dataset.append(vals)\n",
    "    fig.savefig(f\"NN_grid_of_{nentries_wb}\")\n",
    "    #plt.close()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for everything\n",
    "nentries = 10000\n",
    "nfeatures = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUM TO ONE\n",
    "dataset1 = lit.gen_original_data(nentries, nfeatures, dtype='normal')  #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2 = lit.shuffle_dataset(dataset1)\n",
    "\n",
    "dataset_normal= get_wb_dataset(dataset1, dataset2, nentries_wb=100, num_hidden_layers= (nfeatures,3,8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RELATIVITY\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='relativity') #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "dataset_relativity= get_wb_dataset(dataset1, dataset2, nentries_wb=100, num_hidden_layers= (nfeatures,3,8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset1[0])\n",
    "print(dataset_normal[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('nentries:',len(dataset_normal),len(dataset_relativity))\n",
    "print('nfeatures:',len(dataset_normal[0]),len(dataset_relativity[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('shape of entry 1')\n",
    "#print(np.array(dataset_normal[0]).shape)\n",
    "#print(np.array(dataset_relativity[0]).shape)\n",
    "\n",
    "#print('how it should look')\n",
    "#print(dataset1[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## it doesn't need fixing\n",
    "#print('fixed shape of entry 1')\n",
    "#print(np.array([dataset_normal[0]]).T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('nentries, nfeatures')\n",
    "print(dataset1.shape)\n",
    "print(np.array(dataset_normal).shape)\n",
    "print(np.array(dataset_relativity).shape)\n",
    "print()\n",
    "print('shape of entry 1')\n",
    "print(dataset1[0].shape)\n",
    "print(np.array(dataset_normal[0]).shape)\n",
    "print(np.array(dataset_relativity[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures= len(dataset_normal[0])\n",
    "for i in range(15):\n",
    "    print(i)\n",
    "    #plt.figure()\n",
    "    excellent_training(np.array(dataset_normal), np.array(dataset_relativity), num_hidden_layers= (nfeatures,80,50,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gabby making things really complicated Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for everything\n",
    "nentries = 10000\n",
    "nfeatures = 5\n",
    "ntrials= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sum to One\n",
    "print('Sum to One')\n",
    "\n",
    "biases1= []\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='normal')\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "for i in range(ntrials):\n",
    "    w, b= excellent_training(dataset1, dataset2)\n",
    "    biases1.append(b)\n",
    "b1_lists= list(zip(*biases1))\n",
    "\n",
    "## Relativity\n",
    "print('Relativity')\n",
    "\n",
    "biases2= []\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='relativity')\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "for i in range(ntrials):\n",
    "    w, b= excellent_training(dataset1, dataset2)\n",
    "    biases2.append(b)\n",
    "b2_lists= list(zip(*biases2))\n",
    "\n",
    "## NN\n",
    "#for i in range(nfeatures):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NN\n",
    "#for i in range(nfeatures):\n",
    "    #print(i)\n",
    "    #print(b1_lists[i])\n",
    "    #print()\n",
    "    #excellent_training(b1_lists[i], b2_lists[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum to One\n",
    "print('Sum to One')\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='normal')\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "w1, b1= excellent_training(dataset1, dataset2)\n",
    "\n",
    "# Relativity\n",
    "print('Relativity')\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='relativity')\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "w2, b2= excellent_training(dataset1, dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_lists[0][0]\n",
    "b2_lists[0][0]\n",
    "dataset1[0:10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excellent_training(np.array(b1_lists[0]), np.array(b2_lists[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making sure we get a good training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Manual Variables\n",
    "nentries = 10000\n",
    "nfeatures = 4\n",
    "\n",
    "# Datasets\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='normal') #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "alldata,labels= lit.concat_dataset(dataset1, dataset2, wantplots=False)\n",
    "\n",
    "auc = 0\n",
    "w,b = None,None\n",
    "\n",
    "while auc < 0.9:\n",
    "\n",
    "    w, b, auc = lit.neuralnet(dataset1, dataset2, num_hidden_layers=(nfeatures, 3, 8, 4), wantplots=False)\n",
    "\n",
    "    print(f\"auc: {auc}\")\n",
    "\n",
    "# Only draw the network after it find a good training approach\n",
    "lit.draw_network(b, w, figsize=(6, 6), colormap=plt.cm.Greens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Manual Variables\n",
    "nentries = 10000\n",
    "nfeatures = 4\n",
    "\n",
    "# Datasets\n",
    "dataset1= lit.gen_original_data(nentries, nfeatures, dtype='relativity') #dtype args: 'normal', 'squared', 'relativity'\n",
    "dataset2= lit.shuffle_dataset(dataset1)\n",
    "\n",
    "alldata,labels= lit.concat_dataset(dataset1, dataset2, wantplots=False)\n",
    "\n",
    "auc = 0\n",
    "w,b = None,None\n",
    "\n",
    "while auc < 0.9:\n",
    "\n",
    "    w, b, auc = lit.neuralnet(dataset1, dataset2, num_hidden_layers=(nfeatures, 3, 8, 4), wantplots=False)\n",
    "\n",
    "    print(f\"auc: {auc}\")\n",
    "\n",
    "# Only draw the network after it find a good training approach\n",
    "lit.draw_network(b, w, figsize=(6, 6), colormap=plt.cm.Greens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
